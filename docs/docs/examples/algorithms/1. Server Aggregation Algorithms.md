### FedAtt

FedAtt is a server aggregation algorithm, where client updates were aggregated using a layer-wise attention-based mechanism that considered the similarity between the server and client models. The objective was to improve the accuracy or perplexity of the trained model with the same number of communication rounds. In its implementation in `examples/server_aggregation/fedatt/fedatt_algorithm.py`, the default implementation of `aggregate_weights()` is overridden to implement FedAtt as a custom server aggregation algorithm.

```bash
cd examples/server_aggregation/fedatt
uv run fedatt.py -c fedatt_FashionMNIST_lenet5.yml
```

**Reference:** S. Ji, S. Pan, G. Long, X. Li, J. Jiang, Z. Huang. "[Learning Private Neural Language Modeling with Attentive Aggregation](https://arxiv.org/abs/1812.07108)," in Proc. International Joint Conference on Neural Networks (IJCNN), 2019.

---

### FedAdp

FedAdp is another server aggregation algorithm, which exploited the implicit connection between data distribution on a client and the contribution from that client to the global model, measured at the server by inferring gradient information of participating clients. In its implementation in `examples/server_aggregation/fedadp/fedadp_server.py`, the default implementation in `aggregate_deltas()` is overridden to implement FedAdp as a custom server aggregation algorithm.

```bash
cd examples/server_aggregation/fedadp/
uv run fedadp.py -c fedadp_FashionMNIST_lenet5.yml
```

**Reference:** H. Wu, P. Wang. "[Fast-Convergent Federated Learning with Adaptive Weighting](https://ieeexplore.ieee.org/abstract/document/9442814)," in IEEE Trans. on Cognitive Communications and Networking (TCCN), 2021.

---

### FedNova

FedNova addresses the objective inconsistency problem in heterogeneous federated optimization, where clients may perform different numbers of local training epochs. The algorithm normalizes local updates based on the number of local steps taken, enabling effective aggregation across heterogeneous clients. In its implementation in `examples/server_aggregation/fednova/fednova_server.py`, the default implementation of `aggregate_deltas()` is overridden to implement FedNova's normalized averaging. The custom client in `fednova_client.py` randomly varies the number of local epochs and communicates this to the server.

Here's how FedNova works. On the client side, each client randomly selects the number of local epochs (between 2 and `max_local_epochs`) and trains accordingly. The number of epochs is included in the report sent to the server. The server then computes the effective number of steps (`tau_eff`) across all clients and normalizes each client's update by their individual number of local epochs, ensuring fair aggregation.

```bash
cd examples/server_aggregation/fednova/
uv run fednova.py -c fednova_MNIST_lenet5.yml
```

Key configuration parameters:

- `algorithm.max_local_epochs`: Maximum number of local epochs (default: 10)
- `algorithm.pattern`: Pattern for selecting local epochs (`uniform_random` or `constant`)
- `trainer.epochs`: Base number of epochs (used when pattern is `constant`)

**Reference:** J. Wang, Q. Liu, H. Liang, G. Joshi, H. V. Poor. "[Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization](https://proceedings.neurips.cc/paper/2020/hash/564127c03caab942e503ee6f810f54fd-Abstract.html)," in Proc. NeurIPS, 2020.
