# maskcrypt_server.py
import torch
import numpy as np
from typing import Dict, Any, List
from config_manager import ConfigManager
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import warnings
from collections import defaultdict

warnings.filterwarnings("ignore", category=UserWarning)


class MaskCryptServer:
    def __init__(self, config: ConfigManager):
        self.config = config
        self.client_info = {}
        self.client_clusters = {}  # ç”± SnapCFL é¢„èšç±»ç”Ÿæˆ
        self.global_weights = None
        self.current_round = 0
        self.byzantine_set = set(config.get('byzantine_clients', []))
        self.byzantine_selection_count = 0

    def set_global_weights(self, weights: Dict[str, torch.Tensor]):
        self.global_weights = weights

    def process_client_data(self, client_id: int, data: Dict[str, Any]):
        if data is None:
            return
        self.client_info[client_id] = {
            'local_weights': data.get('local_weights'),
            'weight_update': data.get('weight_update'),
            'cluster_id': self.client_clusters.get(client_id, 0)
        }

    def clear_client_info(self):
        self.client_info.clear()

    def increment_round(self):
        self.current_round += 1

    def _simple_average(self, updates_list: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        avg_delta = {}
        for key in updates_list[0].keys():
            avg_delta[key] = torch.stack([upd[key] for upd in updates_list]).mean(dim=0)
        return avg_delta

    def _compute_similarity_matrix(self, all_clients: List[Any]):
        """
        å®ç° SnapCFL è®ºæ–‡ä¸­çš„é¢„èšç±»æ ¸å¿ƒï¼š
        å¯¹æ¯ä¸€å¯¹å®¢æˆ·ç«¯ (i, j)ï¼Œè®­ç»ƒäºŒåˆ†ç±»å™¨åˆ¤æ–­å…¶æ•°æ®æ˜¯å¦æ¥è‡ªåŒä¸€åˆ†å¸ƒã€‚
        åˆ†ç±»å‡†ç¡®ç‡è¶Šé«˜ï¼Œè¯´æ˜åˆ†å¸ƒè¶Šä¸åŒã€‚
        """
        n = len(all_clients)
        similarity_matrix = np.full((n, n), 0.5)  # é»˜è®¤éšæœºæ°´å¹³
        client_ids = [c.client_id for c in all_clients]

        print(f"  ğŸ” æ„å»º {n}x{n} ç›¸ä¼¼æ€§çŸ©é˜µï¼ˆå…± {n * (n - 1) // 2} å¯¹ï¼‰...")

        for i in range(n):
            for j in range(i + 1, n):
                client_i = all_clients[i]
                client_j = all_clients[j]

                try:
                    # è·å–å°‘é‡æ˜æ–‡æ•°æ®ï¼ˆä»…ç”¨äºé¢„èšç±»ï¼‰
                    X_i, y_i = client_i.get_data(max_samples=200)
                    X_j, y_j = client_j.get_data(max_samples=200)

                    # åˆå¹¶æ•°æ®å¹¶æ‰“ä¼ªæ ‡ç­¾ï¼šclient_i â†’ 0, client_j â†’ 1
                    X_combined = torch.cat([X_i, X_j], dim=0)
                    X_flat = X_combined.view(X_combined.size(0), -1).cpu().numpy()
                    y_pseudo = np.array([0] * X_i.size(0) + [1] * X_j.size(0))

                    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
                    if len(np.unique(y_pseudo)) < 2:
                        acc = 0.5
                    else:
                        X_train, X_test, y_train, y_test = train_test_split(
                            X_flat, y_pseudo, test_size=0.3,
                            stratify=y_pseudo, random_state=42
                        )

                        # æ ‡å‡†åŒ– + è®­ç»ƒè½»é‡çº§åˆ†ç±»å™¨
                        scaler = StandardScaler()
                        X_train_scaled = scaler.fit_transform(X_train)
                        X_test_scaled = scaler.transform(X_test)

                        clf = LogisticRegression(max_iter=1000, random_state=42)
                        clf.fit(X_train_scaled, y_train)
                        acc = clf.score(X_test_scaled, y_test)

                    # å­˜å‚¨å‡†ç¡®ç‡ï¼ˆä½œä¸º dissimilarity åº¦é‡ï¼‰
                    similarity_matrix[i][j] = acc
                    similarity_matrix[j][i] = acc

                except Exception as e:
                    print(f"    âš ï¸ å®¢æˆ·ç«¯ {client_i.client_id}-{client_j.client_id} å¤±è´¥: {e}")
                    # ä¿æŒé»˜è®¤ 0.5

        return similarity_matrix, client_ids

    def update_clusters_with_snapcfl(self, all_clients: List[Any]):
        """
        æ‰§è¡Œ SnapCFL é¢„èšç±»ï¼ˆä»…è°ƒç”¨ä¸€æ¬¡ï¼‰
        """
        print("\nğŸ” [SnapCFL é¢„èšç±»] åŸºäºæ•°æ®åˆ†å¸ƒç›¸ä¼¼æ€§åˆ†ç»„...")
        sim_matrix, client_ids = self._compute_similarity_matrix(all_clients)

        # è½¬æ¢ä¸ºè·ç¦»çŸ©é˜µï¼šdistance = |acc - 0.5|
        distance_matrix = np.abs(sim_matrix - 0.5)

        # ä½¿ç”¨ DBSCAN èšç±»ï¼ˆeps å¯æ ¹æ®æ•°æ®è°ƒæ•´ï¼‰
        clustering = DBSCAN(eps=0.15, min_samples=2, metric='precomputed')
        cluster_labels = clustering.fit_predict(distance_matrix)

        # æ˜ å°„å› client_id
        for idx, cid in enumerate(client_ids):
            self.client_clusters[cid] = int(cluster_labels[idx])

        # ç»Ÿè®¡ç»“æœ
        unique_labels = set(cluster_labels)
        num_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
        noise_points = list(cluster_labels).count(-1)

        print(f"  âœ… èšç±»å®Œæˆï¼š{num_clusters} ä¸ªç°‡ï¼Œ{noise_points} ä¸ªå™ªå£°ç‚¹")
        for label in sorted(unique_labels):
            count = list(cluster_labels).count(label)
            members = [cid for idx, cid in enumerate(client_ids) if cluster_labels[idx] == label]
            print(f"    Cluster {label}: {count} clients â†’ {sorted(members)}")

    def _krum_select(self, updates: List[Dict[str, torch.Tensor]], client_ids: List[int], f: int = 1) -> Dict[
        str, torch.Tensor]:
        """
        åœ¨ç»™å®šçš„æ›´æ–°åˆ—è¡¨ä¸­ä½¿ç”¨Krumç®—æ³•é€‰æ‹©ä¸€ä¸ªæœ€å¯ä¿¡çš„æ›´æ–°
        """
        n = len(updates)
        if n == 0:
            return None
        if n == 1:
            return updates[0]

        if n >= 2 * f + 2:
            # Krum é€‰æ‹©
            flat_updates = []
            for upd in updates:
                vec = torch.cat([v.flatten().float() for v in upd.values()])
                flat_updates.append(vec)
            flat_updates = torch.stack(flat_updates)

            diff = flat_updates.unsqueeze(1) - flat_updates.unsqueeze(0)
            distances = torch.sum(diff ** 2, dim=2)

            scores = []
            for i in range(n):
                dists = distances[i].clone()
                dists[i] = float('inf')
                topk_vals, _ = torch.topk(dists, k=n - f - 2, largest=False)
                scores.append(topk_vals.sum().item())

            selected_idx = int(np.argmin(scores))
            selected_update = updates[selected_idx]
            selected_cid = client_ids[selected_idx]

            if selected_cid in self.byzantine_set:
                self.byzantine_selection_count += 1
                print(f"    â— ç°‡å†…Krumé€‰ä¸­æ‹œå åº­èŠ‚ç‚¹ {selected_cid}ï¼ˆç´¯è®¡: {self.byzantine_selection_count}ï¼‰")
            else:
                print(f"    âœ… ç°‡å†…Krumé€‰æ‹©å®¢æˆ·ç«¯: {selected_cid}")

            return selected_update
        else:
            print(f"    âš ï¸ ç°‡å†…å®¢æˆ·ç«¯å¤ªå°‘ ({n} < {2 * f + 2})ï¼Œä½¿ç”¨å¹³å‡ Î”w")
            return self._simple_average(updates)

    def aggregate_with_clustered_krum(self, f: int = 1) -> Dict[str, torch.Tensor]:
        """
        æ–°çš„èšåˆç­–ç•¥ï¼š
        1. åœ¨æ¯ä¸ªç°‡å†…ä½¿ç”¨Krumé€‰æ‹©æœ€å¯ä¿¡çš„æ›´æ–°
        2. åœ¨ç°‡é—´ä½¿ç”¨å¹³å‡èšåˆå¾—åˆ°å…¨å±€æ¨¡å‹
        """
        if not self.client_info or self.global_weights is None:
            print("âš ï¸ æ— å®¢æˆ·ç«¯æ•°æ®æˆ–å…¨å±€æƒé‡ï¼Œè·³è¿‡èšåˆ")
            return self.global_weights

        # æ­¥éª¤1ï¼šæŒ‰ç°‡åˆ†ç»„å®¢æˆ·ç«¯æ›´æ–°
        cluster_updates = defaultdict(list)
        cluster_client_ids = defaultdict(list)

        for cid, info in self.client_info.items():
            if info and info.get('weight_update') is not None:
                cluster_id = info.get('cluster_id', 0)
                update = info['weight_update']

                # ç¡®ä¿æ‰€æœ‰æ›´æ–°éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
                if len(cluster_updates[cluster_id]) == 0 and update:
                    target_device = next(iter(update.values())).device

                aligned_update = {k: v.to(target_device) for k, v in update.items()}
                cluster_updates[cluster_id].append(aligned_update)
                cluster_client_ids[cluster_id].append(cid)

        if not cluster_updates:
            return self.global_weights

        # æ­¥éª¤1.5ï¼šè®¡ç®—å¹¶æ‰“å°æ¯ä¸ªç°‡å†…å®¢æˆ·ç«¯æœ¬åœ°æ¨¡å‹ä¸å…¨å±€æ¨¡å‹çš„ç›¸ä¼¼åº¦
        print(f"  ğŸ”„ å¼€å§‹åˆ†å±‚èšåˆï¼š{len(cluster_updates)} ä¸ªç°‡")

        # è®¡ç®—ç›¸ä¼¼åº¦çš„è¾…åŠ©å‡½æ•° - è®¡ç®—æœ¬åœ°æ¨¡å‹æƒé‡ä¸å…¨å±€æ¨¡å‹æƒé‡çš„ç›¸ä¼¼åº¦
        def compute_model_similarity(local_weights, global_weights):
            try:
                # å±•å¹³æœ¬åœ°æ¨¡å‹æƒé‡
                local_vec = torch.cat([v.flatten() for v in local_weights.values()])

                # å±•å¹³å…¨å±€æ¨¡å‹æƒé‡
                global_vec = torch.cat([v.flatten() for v in global_weights.values()])

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                cos_sim = torch.nn.functional.cosine_similarity(
                    local_vec.unsqueeze(0),
                    global_vec.unsqueeze(0),
                    dim=1
                )
                return cos_sim.item()
            except:
                return 0.0

        # å¯¹æ¯ä¸ªç°‡è®¡ç®—ç›¸ä¼¼åº¦
        for cluster_id in sorted(cluster_updates.keys()):
            client_ids = cluster_client_ids[cluster_id]
            similarities = []
            byzantine_similarities = []
            normal_similarities = []

            for cid in client_ids:
                local_weights = self.client_info[cid].get('local_weights')
                if local_weights is not None and self.global_weights is not None:
                    # ç¡®ä¿å¼ é‡åœ¨ç›¸åŒè®¾å¤‡ä¸Š
                    local_weights_cpu = {k: v.cpu() for k, v in local_weights.items()}
                    global_weights_cpu = {k: v.cpu() for k, v in self.global_weights.items()}

                    # è®¡ç®—æœ¬åœ°æ¨¡å‹ä¸å…¨å±€æ¨¡å‹çš„ç›¸ä¼¼åº¦
                    sim = compute_model_similarity(local_weights_cpu, global_weights_cpu)
                    similarities.append(sim)

                    if cid in self.byzantine_set:
                        byzantine_similarities.append((cid, sim))
                    else:
                        normal_similarities.append((cid, sim))

            if similarities:
                avg_sim = np.mean(similarities)
                min_sim = np.min(similarities)
                max_sim = np.max(similarities)

                print(f"    ğŸ“Š ç°‡ {cluster_id} æ¨¡å‹ç›¸ä¼¼åº¦ç»Ÿè®¡:")
                print(f"      å¹³å‡å€¼: {avg_sim:.4f}, æœ€å°å€¼: {min_sim:.4f}, æœ€å¤§å€¼: {max_sim:.4f}")

                # æ‰“å°æ­£å¸¸å®¢æˆ·ç«¯
                if normal_similarities:
                    print(f"      æ­£å¸¸å®¢æˆ·ç«¯ ({len(normal_similarities)}ä¸ª): ", end="")
                    for cid, sim in normal_similarities:
                        print(f"c{cid}:{sim:.3f} ", end="")
                    print()

                # æ‰“å°æ‹œå åº­å®¢æˆ·ç«¯
                if byzantine_similarities:
                    print(f"      æ‹œå åº­å®¢æˆ·ç«¯ ({len(byzantine_similarities)}ä¸ª): ", end="")
                    for cid, sim in byzantine_similarities:
                        print(f"c{cid}:{sim:.3f} ", end="")
                    print()

        # æ­¥éª¤2ï¼šåœ¨æ¯ä¸ªç°‡å†…ä½¿ç”¨Krumé€‰æ‹©å¯ä¿¡æ›´æ–°
        cluster_selected_updates = []

        for cluster_id, updates in cluster_updates.items():
            client_ids = cluster_client_ids[cluster_id]
            print(f"    ğŸ“Š å¤„ç†ç°‡ {cluster_id}: {len(updates)} ä¸ªå®¢æˆ·ç«¯")

            selected_update = self._krum_select(updates, client_ids, f)
            if selected_update is not None:
                cluster_selected_updates.append(selected_update)

        # æ­¥éª¤3ï¼šåœ¨ç°‡é—´ä½¿ç”¨å¹³å‡èšåˆ
        if not cluster_selected_updates:
            print("âš ï¸ æ‰€æœ‰ç°‡éƒ½æ— æœ‰æ•ˆæ›´æ–°ï¼Œä¿æŒåŸæƒé‡")
            return self.global_weights

        print(f"  ğŸ”„ ç°‡é—´èšåˆï¼š{len(cluster_selected_updates)} ä¸ªç°‡çš„æ›´æ–°")
        final_delta = self._simple_average(cluster_selected_updates)

        # æ­¥éª¤4ï¼šåº”ç”¨æ›´æ–°åˆ°å…¨å±€æƒé‡
        new_weights = {
            key: self.global_weights[key] + final_delta[key]
            for key in self.global_weights
        }

        print(f"  âœ… åˆ†å±‚èšåˆå®Œæˆ")
        return new_weights